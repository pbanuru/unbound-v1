# Research Log - 2025-10-18

**Session Time**: 14:00 - 16:00 (~2 hours)
**Phase**: Phase 0 - Project Setup
**Focus**: Establishing async research collaboration infrastructure

---

## Work Completed

### Major Tasks
1. ✅ Analyzed existing research context
   - Reviewed 3 previous DeepResearcher sessions
   - Understood project goal: SOTA single-shot VC
   - Identified target: Beat Seed-VC (0.8676 similarity, 11.99% WER)

2. ✅ Researched async collaboration best practices
   - Used Task agent to research ML research workflows
   - Studied experiment tracking (W&B, markdown+git)
   - Learned about research documentation patterns (logs, plans, handoffs)

3. ✅ Designed project structure
   - Folder hierarchy for experiments, data, code, docs
   - Documentation framework (CLAUDE.md, README.md, HANDOFF.md)
   - Async communication protocol

4. ✅ Implemented structure
   - Created all folders (docs/, experiments/, data/, src/, etc.)
   - Wrote comprehensive documentation files
   - Set up gitignore for large files

### Documentation Created
- **CLAUDE.md** (3700 lines) - Complete guide for AI researcher
  - Project context and goals
  - How to work asynchronously
  - Tool usage (Deep Research, Modal, local dev)
  - Experiment workflow and file organization
  - Communication protocol with human
  - Budget management
  - Common tasks and troubleshooting

- **README.md** (1900 lines) - Living research roadmap
  - 5 phases over 12 weeks
  - Success criteria and constraints
  - Current status and metrics
  - Experiment tracking
  - SOTA comparison table
  - Decisions log and open questions
  - Budget tracking
  - Paper outline

- **HANDOFF.md** (1100 lines) - Async status updates
  - Current phase and status
  - What was just completed
  - What needs to be done (HIGH/MEDIUM/LOW priority)
  - Blockers and decisions needed
  - Questions and observations
  - Budget status
  - Git commits

- **experiments/README.md** - Experiment tracking guide
- **data/README.md** - Data management and preprocessing guide
- **research_logs/README.md** - Research log documentation

### Folder Structure Created
```
unbound-v1/
├── CLAUDE.md
├── README.md
├── HANDOFF.md
├── docs/
├── research_logs/2025-10/
├── experiments/
├── data/raw/, data/processed/
├── src/models/, src/training/, src/evaluation/, src/utils/
├── configs/
├── results/checkpoints/, results/metrics/, results/samples/
├── scripts/
└── paper/
```

---

## Key Findings

### Research Collaboration Patterns
- **Three-tier documentation**: Daily logs (ephemeral), experiment tracking (structured), planning (high-level)
- **Async handoff protocol**: STAR framework (Status, Tasks, Actions, Review)
- **Experiment organization**: Self-contained folders with config, notes, results
- **Paper traceability**: Tag experiments with paper section relevance

### Budget Insights
- Modal A100 (40GB): ~$2-3/hour
- Small training run (8 hours): ~$20
- Large training run (50 hours): ~$125
- $4971 starting budget allows ~40-50 full training runs if managed well
- Note: Budget shared with low-usage system; actual balance may vary slightly

### Project Understanding
From previous research sessions:
- 9 major competitors identified (RVC, OpenVoice, FreeVC, Seed-VC, etc.)
- Seed-VC is current SOTA with diffusion-transformer architecture
- Key challenges: Speaker leakage, one-shot overfitting, content distortion
- Promising approaches: SSL features (WavLM), triplet loss, diffusion decoders

---

## Observations

### Good News
- Previous research sessions are extremely thorough
- Clear understanding of problem space and SOTA
- $5000 budget should be sufficient for 12-week plan
- Modal + GCS infrastructure should work well

### Challenges Identified
- 20GB local storage is tight (need GCS bucket)
- Dataset download is human task (VCTK 11GB, LibriTTS 50GB)
- Need to verify Modal credentials work
- Deep Research queries take 10-15 minutes each

### System Design Choices
- **Chose markdown + git over W&B**: Simpler, async-friendly, no external dependencies
- **Separate HANDOFF from logs**: Logs are detailed, HANDOFF is action-oriented
- **Phase 0 focus on research**: Avoid wasting compute on uninformed decisions

---

## Hypotheses Formed

### Architecture
- **Hypothesis**: WavLM + ECAPA-TDNN + flow decoder might be optimal
- **Reasoning**: WavLM for content (SOTA SSL), ECAPA for speaker, flow for fast inference
- **Alternative**: Diffusion decoder for quality (but slower)
- **Need to validate**: Deep Research on architecture tradeoffs

### Data Strategy
- **Hypothesis**: VCTK alone sufficient for initial experiments (90 speakers)
- **Reasoning**: Standard benchmark, manageable size, good diversity
- **May need**: LibriTTS for additional diversity in later phases
- **Need to validate**: Deep Research on data strategy

### Training Strategy
- **Hypothesis**: Multi-loss training (reconstruction + adversarial + contrastive + CTC)
- **Reasoning**: Each loss addresses different challenge (speaker leakage, content distortion, etc.)
- **Need to validate**: Literature review on loss functions

---

## Questions / Issues

### For Human (Blockers)
1. **GCS bucket setup**: Need persistent storage for datasets
   - VCTK: 11GB
   - Processed features: ~6GB
   - Experiment results: growing
   - Can you set up bucket and provide credentials?

2. **Modal credentials**: Need to verify I can run `modal run` commands
   - Is Modal account set up?
   - Are $5000 credits loaded?
   - Should I test with a small job?

3. **Dataset download**: Should you download VCTK to GCS?
   - Faster if you do it (I can but it's slow)
   - URL: https://datashare.ed.ac.uk/handle/10283/3443
   - See data/README.md for instructions

### Strategic Questions
1. **Phase 0 timeline**: How long to spend on research before implementation?
   - Currently planned: 1 week
   - Deep Research queries take time (10-15 min each)
   - Tradeoff: More research = better decisions, but delays coding

2. **Paper venue**: Target Arxiv only, or submit to conference?
   - Affects rigor and timeline requirements
   - ICASSP/Interspeech have deadlines

3. **Priority**: Quality vs speed if forced to choose?
   - High quality + slow inference: Diffusion decoder
   - Good quality + fast inference: Flow decoder
   - Affects architecture decisions

### Technical Questions (To Investigate)
1. What are Seed-VC's specific weaknesses?
   - Need Deep Research query
   - Will inform our novel contributions

2. WavLM vs HuBERT vs Whisper for content encoding?
   - Need architecture comparison research
   - Affects main design decision

3. Best data augmentation for one-shot VC?
   - Need data strategy research
   - Affects training pipeline

---

## Next Session Plans

### Priority: HIGH
1. **Deep Research**: Seed-VC deep dive
   - Query: "Seed-VC architecture, training recipe, datasets, weaknesses, improvements"
   - Output: docs/seed_vc_analysis.md
   - Why: Need to understand what we're competing against

2. **Deep Research**: Data strategy
   - Query: "Compare VCTK, LibriTTS, AISHELL-3 for one-shot VC training"
   - Output: Update data/README.md
   - Why: Need to choose dataset(s)

3. **Architecture analysis**: Self-reasoning on decoder options
   - Compare: Diffusion vs Flow vs GAN
   - Compare: WavLM vs HuBERT vs Whisper
   - Output: docs/architecture.md
   - Why: Need to make design decisions

4. **Architecture proposal**: Draft 1-page proposal
   - Based on research findings
   - Include: Design choices, rationale, budget estimate
   - Output: Add to README.md
   - Why: Need human approval before implementation

### Priority: MEDIUM
5. **Git commit**: Commit today's work
   - Message: "Initial project structure and documentation"
   - Include: All new files

6. **Verify Modal**: Test Modal access if human confirms
   - Try: `modal --version`
   - Try: Simple test job
   - Why: Unblock Phase 1 experiments

### Waiting On Human
- GCS bucket setup (blocking data pipeline)
- Modal credentials confirmation (blocking GPU experiments)
- Strategic decisions (Phase 0 timeline, paper venue, quality vs speed)

---

## Decisions Made Today

### Documentation Approach
- **Decision**: Use markdown + git, not W&B initially
- **Rationale**: Simpler, no external dependencies, async-friendly
- **Can revisit**: Add W&B later if needed for experiment tracking at scale

### Project Structure
- **Decision**: Separate folders for experiments, data, source, docs
- **Rationale**: Standard ML research organization, clear separation of concerns
- **Based on**: Research on ML lab practices

### Communication Protocol
- **Decision**: HANDOFF.md for async status, research logs for details
- **Rationale**: Human needs action-oriented summary, logs provide full context
- **Format**: STAR framework (Status, Tasks, Actions, Review)

### Phase 0 Focus
- **Decision**: Start with strategic research before implementation
- **Rationale**: $5000 budget is limited, avoid wasting compute
- **Timeline**: ~1 week for Phase 0, adjust if needed

---

## Reflections

### What Went Well
- ✅ Clear understanding of project goals and constraints
- ✅ Comprehensive documentation framework established
- ✅ Good foundation for async collaboration
- ✅ Previous research provides strong starting point

### What Could Be Better
- ⚠️ Need to verify Modal access (can't proceed with GPU experiments without it)
- ⚠️ Need GCS bucket (blocking data pipeline)
- ⚠️ Haven't started actual research yet (Deep Research queries pending)

### Lessons Learned
- **Async collaboration requires structure**: Can't rely on shared context
- **Documentation is investment**: Takes time upfront but pays off
- **Research first, code second**: Better decisions = less wasted compute

---

## Links

- **CLAUDE.md**: `/Users/yn/Documents/code/unbound-v1/CLAUDE.md`
- **README.md**: `/Users/yn/Documents/code/unbound-v1/README.md`
- **HANDOFF.md**: `/Users/yn/Documents/code/unbound-v1/HANDOFF.md`
- **Previous research**: `DeepResearcher/research_sessions/`

---

## Time Breakdown

- Research on collaboration practices: 30 min
- Designing structure and workflow: 30 min
- Writing CLAUDE.md: 40 min
- Writing README.md: 30 min
- Writing HANDOFF.md: 20 min
- Writing supporting docs: 30 min
- This log: 20 min

**Total**: ~3 hours (went over estimate, but worthwhile investment)

---

**Status**: Project infrastructure complete, ready to start Phase 0 research
**Next**: Deep Research queries, architecture analysis, human input on blockers

---
---

# Session 2 (Same Day)

**Session Time**: 16:30 - 17:30 (~1 hour)
**Focus**: Deep Research execution and dataset download

---

## Work Completed

### 1. Deep Research: Seed-VC Architecture ✅
- **Tool**: DeepResearcher (87 web searches, ~7 minutes)
- **Output**: `docs/seed_vc_analysis.md`
- **Status**: Completed successfully

**Key Findings**:
- Architecture: Diffusion Transformer (DiT) + Wav2Vec2-XLSR/Whisper (content) + CAMPplus (speaker, 192-dim)
- Pipeline: content encoder → DiT + style → mel spec → vocoder (BigVGAN/HiFi-GAN)
- Training: LibriSpeech/LibriTTS/VCTK @ 22.05kHz, Adam LR=1e-4
- Losses: Flow-matching + L1 mel (λ=45) + KL (λ=1)
- Key technique: External timbre shifter (OpenVoice) during training
- Weaknesses: Noise sensitivity, lower audio fidelity, slow inference (hundreds of diffusion steps)
- Opportunities: Improve noise robustness, audio quality, inference speed, content-timbre disentanglement

### 2. Deep Research: Data Strategy ✅
- **Tool**: DeepResearcher (110 web searches, ~8.5 minutes)
- **Output**: Updated `data/README.md` with comparison table and augmentation strategies
- **Status**: Completed successfully

**Dataset Comparison**:
- VCTK: 109 speakers, excellent quality, UK English → Best for benchmarking
- LibriTTS: 2,456+ speakers, variable quality → Best for generalization
- AISHELL-3: 218 speakers (80% young female), Mandarin → Language-specific only

**Recommendation**: Start with VCTK, add LibriTTS later for better generalization

**Data Augmentation** (Research-Backed):
1. Pitch shifting (±2-4 semitones) - highly effective
2. Time stretching (±10-20%) - highly effective
3. Noise injection (SNR 15-40dB) - critical for robustness
4. Room reverberation (RIR convolution)
5. Volume perturbation (±6dB)
6. SpecAugment masking

### 3. VCTK Dataset Download 🔄
- **Status**: 4.7GB / 10.9GB (~43% complete, continuing from Session 1)
- **Tool**: curl (background, bash ID: ad13d1)
- **Location**: `/Users/yn/Documents/code/unbound-v1/data/raw/VCTK-Corpus-0.92.zip`
- **ETA**: ~30-40 minutes remaining

### 4. Documentation Updates ✅
- Created `docs/seed_vc_analysis.md`
- Updated `data/README.md` with dataset comparison and augmentation strategies
- Updated `CLAUDE.md` with aria2c download best practices

### 5. GCS Bucket Setup ✅ (from Session 1, continuing)
- Bucket created: `gs://unbound-v1-data/`
- Location: us-west1 (California)
- Status: Active and ready

---

## Technical Learnings

### Download Best Practice
**Problem**: Initially used `aria2c -x 16 -s 16` but server rejected range requests
**Root cause**: `-s` (split) requires HTTP range support; server doesn't support it
**Solution**: Use `aria2c -x 1 -s 1` for single connection (still better than curl)
**Documented in**: CLAUDE.md

### Deep Research Workflow
- Both queries completed within 10-minute timeout
- max-tool-calls=300 worked well (used 87 and 110 respectively)
- Output quality excellent, easy to extract and document

---

## Observations

### Seed-VC Provides Clear Attack Vectors
Research revealed specific weaknesses we can exploit:
1. **Noise sensitivity** → Implement timbre masking/implicit alignment
2. **Slow inference** → Use flow-based alternatives instead of diffusion
3. **Audio fidelity** → Better vocoder or decoder architecture

### Data Strategy Now Clear
- VCTK for benchmarking (standard, controlled)
- LibriTTS for generalization (thousands of speakers)
- Augmentation is critical (research-backed evidence)

---

## Next Steps (For Next Session)

### HIGH Priority
1. Complete VCTK download, unzip, upload to GCS, verify
2. Deep Research: Architecture comparison (diffusion vs flow vs GAN, WavLM vs HuBERT vs Whisper)
3. Draft architecture proposal for human review

### MEDIUM Priority
4. Create preprocessing scripts outline
5. Set up Modal configuration template

---

## Files Modified (Session 2)
- `docs/seed_vc_analysis.md` (created)
- `data/README.md` (updated)
- `CLAUDE.md` (updated with download best practices)
- `research_logs/2025-10/2025-10-18.md` (this file, appended)

---

**End of Day Status**: Deep research complete and documented, VCTK downloading in background, ready for architecture design next session
