# WavLM as a VC content encoder  
**Content quality:**  WavLM (especially Large) is designed to capture rich acoustic and phonetic information. It builds on HuBERT idea but adds noise‐robust, denoising pretraining ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=framework%2C%20where%20some%20of%20the,WavLM%20model%20consists%20of%20a)). In practice, WavLM embeddings preserve phonetic content well. For example, W2VC (Huang et al. 2023) uses a WavLM “universal speech representation” (WLMR) as the content feature and demonstrates much lower mel-cepstral distortion (MCD≈8.90) and higher MOS on naturalness (4.45) and speaker similarity (3.62) than traditional mel-spectrogram baselines ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=show%20that%20GRL%20can%20purify,is%20superior%20to%20the%20baseline)). This suggests WavLM’s content encoder yields high-fidelity phonetic representation. In general, hidden states from the top layers of WavLM carry the majority of the phonetic information. In fact, Matassoni et al. (2024) found that using the final hidden-layer output (768‐dim for WavLM base) gives a very good content representation ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=characteristics%20but%20still%20maintain%20some,768%20as%20speech%20embeddings)). (Earlier HuBERT layers can also be used – e.g. WavLM pretraining used HuBERT’s 9th layer clusters ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=WLMR%3A%20In%20the%20experiments%2C%20we,dimensional%20bottleneck%20features))­ – indicating mid-level layers capture phonetic content.)  

**Speaker leakage:**  By default, WavLM embeddings retain noticeable speaker traits: the 768-d hidden state still contains sufficient speaker identity that an adversary can partly recognize the speaker ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=characteristics%20but%20still%20maintain%20some,768%20as%20speech%20embeddings)).  Thus, a disentanglement step is usually required.  For example, W2VC inserts a gradient-reversal adversary and CTC supervision on the content encoder to “purify” WavLM features of speaker identity ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=W2VC%20consists%20of%20three%20parts%3A,Moreover%2C%20the%20synthesized%20speech)).  Their results show that this adversarial purification significantly removes speaker cues from the content stream. In other work on anonymization, HuBERT/WavLM content vectors also needed contrastive or bottleneck processing to hide speakers ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=presents%20the%20performance%20of%20the,using%20a%20latent%20representation%20inevitably)) ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=characteristics%20but%20still%20maintain%20some,768%20as%20speech%20embeddings)). In practice, best results come from extracting content from WavLM and then explicitly removing speaker features (via adversarial loss, CTC, or bottleneck quantization). In summary, raw WavLM features are not speaker-agnostic; one must combine them with a speaker-disentanglement mechanism.  

**Accent/noise/style robustness:**  WavLM was pretrained on very large, diverse data *with noise augmentation*. Its masked-prediction objective includes a denoising task (mixing two waveforms, adding noise to 50% of frames) to predict the uncorrupted speech ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=framework%2C%20where%20some%20of%20the,WavLM%20model%20consists%20of%20a)).  This makes WavLM inherently robust to background noise and some forms of signal corruption. Anecdotally, WavLM handles a variety of accents and speaking styles well, since its 94k-hour Libri-light+ data and noise-corruption training induce invariances. It is not explicitly multilingual (trained on English), so extreme accents or new languages may degrade performance, but it is generally more robust than older SSL encoders. 

**Efficiency/latency:**  WavLM Large and Base are large Transformer models (hundreds of millions of parameters).  Real-time use on CPU is unlikely; even with GPU the forward pass is relatively slow.  For example, WavLM Base (768-dim) has ~94M params, WavLM Large (~1024-dim) is ~317M.  This is lighter than Whisper’s largest models, but still heavy. Inference latency (real-time factor) is often >1 (slower than real time) unless heavily optimized. For lower latency, one might use the smaller WavLM Base or techniques like layer reduction, but with some loss in content quality.  

**One-shot VC performance:**  Empirically, WavLM-based VC has been shown to excel. In Huang et al. (2023), their WavLM-based VC (W2VC) significantly outperformed earlier baselines: e.g. *FragmentVC* (wav2vec2-based) and conventional VAE models had higher MCD and lower MOS ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=the%20table%2C%20it%20can%20be,embedding%20dictionary%20or%20only%20introducing)). The reported objective/subjective scores (MCD=8.901; MOS nat.~4.45) were better than historic benchmarks on VCTK/CMU-ARCTIC ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=show%20that%20GRL%20can%20purify,is%20superior%20to%20the%20baseline)). (No direct head-to-head with HuBERT was given, but the implication is WavLM provided state-of-art voice conversion quality at that time.) We are not aware of contrasting results using HuBERT in one-shot VC, suggesting WavLM is preferred. 

**Layer choice:**  Best practice is to use the high-level hidden states of WavLM.  Matassoni et al. used “WavLM HS-768” (i.e. the last hidden-layer output, dimension 768 for WavLM Base) as the content vector ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=characteristics%20but%20still%20maintain%20some,768%20as%20speech%20embeddings)).  In practice, “HS” (Hidden State) representations from the last few Transformers layers encode phonetic details with some context and perform well. Some systems may even average or linearly combine multiple top layer outputs to smooth representations, but often simply using the topmost layer (possibly projected or bottlenecked) is effective.  

# HuBERT as a VC content encoder  
**Content quality:**  HuBERT (Base or Large) also provides strong phonetic features, being a predecessor of WavLM. HuBERT’s training (masked prediction of clustered acoustic units) yields embeddings that capture detailed phonetic and word-level content. However, its phonetic representations may be slightly less robust than WavLM’s denoised outputs. In voice conversion contexts, hidden states from HuBERT similarly encode the linguistic content well – akin to WavLM’s hidden layers.  In practice, teams have used HuBERT hidden features or CTC token posteriors as content; both preserve fine timing information. ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=presents%20the%20performance%20of%20the,using%20a%20latent%20representation%20inevitably)).  

**Speaker leakage:**  Like WavLM, HuBERT features contain speaker identity. In Matassoni et al. (2024), HuBERT base with last-layer output (1024 dims) was used as an anonymization embedding ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=characteristics%20but%20still%20maintain%20some,768%20as%20speech%20embeddings)).  They found (as expected) that the latent (hidden) HuBERT features leak speaker traits. Using its CTC head (phoneme posteriorgrams) masked most speaker info, but at the cost of content fidelity. Thus, HuBERT similarly requires extrinsic disentanglement (adversarial loss or bottleneck) to remove speaker. Without it, converted speech will still sound like the source except for the re-synthesized voice. 

**Accent/noise/style robustness:**  HuBERT was trained on English audiobooks without artificial noise. It is **less** inherently robust to noise than WavLM or Whisper.  Accents not in LibriSpeech may degrade performance. Ambient noise or overlapping speech will also hurt HuBERT more. It does cover moderate style variation from book readings, but lacks deliberate noise augmentations. In summary, HuBERT’s content encoder is good for clean speech, but one should expect reduced performance under heavy noise or unfamiliar accent. 

**Efficiency/latency:**  HuBERT Base (768-dim, ~95M parameters) is similar in size to WavLM Base and much smaller than WavLM Large. Inference speed is modestly faster, but still requires GPU for low-latency. It is not streaming-friendly (self-attentive, full-context). Compared to Whisper, HuBERT Base is lighter (easier to run) but lacks Whisper’s highly parallel forward pass. HuBERT Large (1024-dim, ~317M params) is heavier and slower. So in practice, if compute is a concern, one might choose HuBERT Base over WavLM Large to save resources – at some cost in content richness. 

**One-shot VC performance:**  To our knowledge, HuBERT has not been demonstrated to yield better voice conversion quality than WavLM.  The anonymization study in Matassoni et al. reported comparative results: HuBERT’s CTC and hidden features gave similar (slightly worse) speaker anonymization performance than WavLM ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=presents%20the%20performance%20of%20the,using%20a%20latent%20representation%20inevitably)).  There is no public report of a HuBERT-based VC model beating WavLM-based systems.  Older VC methods mostly used mel-spectrograms or simpler features. We infer that a HuBERT-based encoder would perform roughly on par with a WavLM Base (perhaps somewhat worse than WavLM Large), but worse than WavLM-enhanced systems.  

**Layer choice:**  Similar to WavLM, best practice is to use HuBERT’s high-level features but possibly reduce dimension. In Matassoni et al., they compared HuBERT HS (last hidden, 1024-d) vs CTC. They found larger HuBERT vectors leak more speaker info ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=characteristics%20but%20still%20maintain%20some,768%20as%20speech%20embeddings)). Thus, one might prefer HuBERT CTC (phoneme posteriors, ~32-dim) if maximal anonymization is needed – though that severely degrades speech naturalness. More realistically, one uses the hidden state from a mid-to-late layer and then possibly weight or bottleneck it. There is no single “best” layer published, but the last layer (or a small layer ensemble) is commonly used for content. If speaker leakage is a problem, one can also consider performing k-means clustering on an intermediate layer (as HuBERT originally did in training) to form a discrete content code, which tends to wash out speaker id. 

# Whisper-based content encoders  
**Content quality:**  The Whisper encoder (OpenAI’s multilingual ASR model) is trained on 680k hours of paired speech–text across many languages. Its latent embeddings are extremely content-rich. In fact, Whisper’s convolutional+Transformer encoder produces representations that cluster strongly by linguistic content (phonemes or words) across diverse speakers and languages ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=Whisper%E2%80%99s%20encoder%20is%20a%20deep,robustness%20to%20acoustic%20domain%20variability)). Whisper embeddings are thus excellent carriers of phonetic/semantic content. They often outperform wav2vec2/WavLM on low-resource ASR tasks ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=Quantitative%20studies%20show%20that%20Whisper,encoders%20deliver)), implying very robust content encoding. In voice conversion, one can use Whisper’s penultimate hidden states or its intermediate features as a content embedding. Unlike WavLM/HuBERT, Whisper was explicitly trained as an encoder–decoder ASR, so its content vectors are very aligned with textual content. A caveat: Whisper’s frame rate is lower (due to convolutional downsampling), so time resolution is coarser (e.g. 10 ms frames). But it still encodes prosody patterns that align to words/syllables effectively. 

**Speaker leakage:**  Remarkably, Whisper’s uppermost layers tend to discard speaker identity. Yang et al. (2023) observed that while Whisper strongly clusters content, speaker traits become weak in the final layers ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=Whisper%E2%80%99s%20encoder%20is%20a%20deep,robustness%20to%20acoustic%20domain%20variability)) ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=,tuning%20can%20be%20computationally%20demanding)).  In other words, Whisper was trained to be content-focused (via ASR loss), so the model actively learns to ignore speaker variation when predicting transcripts. Experiments show that speaker verification on Whisper final embeddings is poor unless one brands content as speaker. For voice conversion, this is a double-edged sword: it means Whisper-derived content features are naturally disentangled from speaker, which is good. But it also means if one wanted to preserve any target speaker cue in the content encoder (e.g. for style transfer), one must use intermediate layers or separate speaker encoders. In practice, one uses Whisper as a **content encoder** and provides a separate speaker embedding (from a speaker encoder) to the decoder. Whisper need not be “disentangled” – it already largely separates content from speaker. If one inadvertently used the very bottom layers of Whisper, some speaker info might leak in, but the last layers have little. 

**Accent/noise/style robustness:**  Whisper is extremely robust to accents, noise, and varied speaking styles. Its training data included speech in 99 languages from many sources, with diverse background noise and speakers. Empirical benchmarks show it generalizes far beyond typical ASR: it works in noisy environments, varied recording conditions, and for unseen accents. The emergentmind review notes “improved domain adaptation and multi-lingual robustness due to the cross-lingual variability in Whisper’s pre-training” ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=established%20baselines%20%28Zhao%20et%20al,aware%20ASR%20using)). Thus, we expect a Whisper-based content encoder to be more fault-tolerant to accent and noise than monolingual SSL models. It can function on very noisy or accented speech with minimal drop in content extraction. Its robustness extends to speaking style (it was trained on podcasts, conversations, etc.), so style variations should also be handled gracefully in the content embeddings. 

**Efficiency/latency:**  Whisper models range from ~50 M (Tiny) up to ~1.5 B (Large-v2).  Even the “small” (244 M) and “medium” (769 M) require significant compute for real-time use. On modern GPUs, Whisper-base (~60M) can run at a few hundred ms for a few seconds of audio (so near real-time), but larger versions will lag unless optimized.  Also, Whisper’s encoder is non-causal (it uses bidirectional attention), so pure streaming is not supported without modification. Recent work (Simul-Whisper variants) can handle streaming, but out of the box Whisper is offline and high-latency. Compared to WavLM/HuBERT, a Whisper-base may be similar to WavLM-large in compute, while Whisper-large is much heavier. In short: Whisper content encoding is currently plausible only in offline or high-latency pipelines, unless one uses the very small variants. 

**One-shot VC performance:**  Explicit VC benchmarks using Whisper are scarce. A related line of work is “whisper-to-normal” (WESPER) conversion, which uses a unit-based encoder (not exactly Whisper’s hidden states) to convert whispered speech ([huggingface.co](https://huggingface.co/papers/2303.01639#:~:text=utterances,converted%20from%20a%20whisper%20was)). But for normal VC (arbitrary source→target), there is no standard evaluation yet. We expect that using Whisper as a content encoder would yield excellent intelligibility (low WER) due to its ASR roots, but we must explicitly add or mix in speaker information for the target voice. In absence of direct studies, we infer that Whisper content features should allow state-of-art content preservation (since it is an ASR engine), but fidelity and similarity will depend on the VOCODER/decoder. No published MOS or MCD for whisper-VC exist, but given the advantages in content encoding, a Whisper-based VC could match or exceed WavLM if implemented carefully (with a proper speaker encoder).  

**Layer choice:**  Best practice with Whisper is to **aggregate multiple layers** rather than use only the final layer.  Studies show that simply taking Whisper’s last embedding neglects speaker/style cues, which one often needs separately. Many works recommend using weighted combinations of intermediate layer outputs for downstream tasks ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=,discriminative%20cues)). For example, one might take a weighted average of layers 10–20 in a 32-layer Whisper to form the content embedding.  This captures robust phonetic features while not fully discarding higher-level information. In fact, Whisper-derived ASR improvements have come from “layer-wise feature aggregation” ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=,discriminative%20cues)). For speaker cues (as a complementary speaker encoder), one could focus on layers ~16–24 (mid-to-late), which have been shown to encode identity ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=%2A%20Partial%20Multi,The%20process%20involves)). In summary, use a *multi-layer fusion* of the Whisper encoder (e.g. an affine combination or concatenation of a few top layers) to get the best content representation. If computation is a concern, one might also extract features at a coarser rate or use the built-in CTC head (which is a token posterior and extremely disentangled, akin to HuBERT’s cluster IDs). 

# Best practices (2024–2025) and layer selection  
- **Model choice:**  Recent VC systems favor robust, content-centric encoders. WavLM Large or Base (e.g. “WavLM-HuBERT” models) are widely used; HuBERT (Base) is a reasonable alternative if resources are limited. Whisper (Base/Small/Medium) is emerging due to its superior content encoding. Whichever model, it’s common to freeze the pre-trained encoder (no fine-tuning) and add simple encoder heads in the VC pipeline.  
- **Disentanglement:**  Always explicitly separate content vs speaker encodings. Use a speaker encoder (e.g. d-vector or x-vector model) alongside the SSL content encoder. If the content encoder leaks speaker info (as with WavLM/HuBERT), apply adversarial loss or information bottlenecks on the content stream ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=W2VC%20consists%20of%20three%20parts%3A,Moreover%2C%20the%20synthesized%20speech)) ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=characteristics%20but%20still%20maintain%20some,768%20as%20speech%20embeddings)). Some groups use an intermediate “projection” layer after the SSL encoder that is trained with a gradient-reversal speaker classifier.  
- **Layer representations:**  The consensus is to use higher-layer outputs for content. For WavLM/HuBERT, the *last hidden state vector* (or a small bottleneck of it) is standard. For Whisper, aggregate multiple upper layers (rather than only the very top). In practice, you might take the sum or mean of the last N Transformer blocks of Whisper (e.g. blocks 24–32 in a 32-layer model) for content, or learn a linear weighted sum of layers (as some experts have). Avoid using only layer-1 or layer-2 (too low-level) or only the final single layer (which loses speaker); mid/top layers give best phonetic content with manageable residual style information ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=,discriminative%20cues)) ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=,tuning%20can%20be%20computationally%20demanding)).  
- **Preprocessing:**  Since WavLM/HuBERT expect raw waveform, it’s straightforward. Whisper expects log-mel input; typically one supplies the same waveform/resample to 16 kHz and feed into Whisper’s preprocessor. Ensure silence/non-speech and too-quiet speech are handled (normalize loudness) because ASR encoders are sensitive to energy.  
- **Latency tricks:**  If streaming/real-time VC is needed, use smaller models (Whisper small or WavLM Base) and chunked processing. Whisper requires special causal masking for streaming (see Simul-Whisper). WavLM also can be chunked at some cost in context. Systems like “StreamVC” show that low-latency conversion is possible by chunkwise encoding plus overlapped windowing, but at some trade-off in quality. For most applications, VC is offline.  
- **Post-encoding:**   Finally, regardless of encoder, always use a robust vocoder (e.g. HiFi-GAN, WaveNet) retrained on the chosen features. W2VC emphasized that training a vocoder on reconstructed WavLM features (instead of mel) improved quality ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=Retrained%20HiFi,fidelity%20audio%20signals.%20In%20our)) ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=the%20table%2C%20it%20can%20be,embedding%20dictionary%20or%20only%20introducing)). Likewise, if using Whisper or HuBERT features, retrain or adapt a vocoder (or use a codec-based approach like EnCodec) to those features for best audio quality.  

In summary, WavLM currently offers the best-reported trade-offs for VC content encoding (high phonetic fidelity and robust performance ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=show%20that%20GRL%20can%20purify,is%20superior%20to%20the%20baseline)) ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=characteristics%20but%20still%20maintain%20some,768%20as%20speech%20embeddings))), HuBERT is a close second (good content but more speaker leakage), and Whisper-based encoders promise superior content preservation and noise robustness (at a computational cost). Using hidden states from the top layers of WavLM/HuBERT or aggregated layers of Whisper, combined with explicit speaker disentangling, is a best practice for one-shot voice conversion as of 2024–2025.  

**Sources:** Recent VC and anonymization studies ([asmp-eurasipjournals.springeropen.com](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00312-8#:~:text=show%20that%20GRL%20can%20purify,is%20superior%20to%20the%20baseline)) ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=presents%20the%20performance%20of%20the,using%20a%20latent%20representation%20inevitably)) ([www.mdpi.com](https://www.mdpi.com/2076-3417/14/9/3876#:~:text=characteristics%20but%20still%20maintain%20some,768%20as%20speech%20embeddings)), and a survey of Whisper-derived encoders ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=Whisper%E2%80%99s%20encoder%20is%20a%20deep,robustness%20to%20acoustic%20domain%20variability)) ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=,discriminative%20cues)) ([www.emergentmind.com](https://www.emergentmind.com/topics/whisper-derived-encoders#:~:text=,tuning%20can%20be%20computationally%20demanding)), provide the above empirical comparisons and recommendations.