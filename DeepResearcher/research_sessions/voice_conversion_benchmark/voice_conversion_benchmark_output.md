# Voice Conversion Benchmarks

Hugging Face currently does **not** offer a dedicated built-in benchmark (leaderboard) specifically for general speech-to-speech voice conversion.  Instead, voice conversion (VC) is typically evaluated via community datasets and challenges.  For example, the **Voice Conversion Challenge (VCC)** series provides standard test sets and evaluation protocols (VCC 2016, 2018, 2020, etc.) ([arxiv.org](https://arxiv.org/html/2306.14422#:~:text=In%20light%20of%20this%2C%20the,and%20to%20share%20views%20about)) ([paperswithcode.com](https://paperswithcode.com/dataset/voice-conversion-challenge-2018#:~:text=2016%20was%20launched%20in%202016,The%20challenge)).  These challenges supply common parallel or non-parallel speech corpora (often subsets of VCTK or other multi-speaker databases) and use large-scale listening tests to rank systems by naturalness and speaker similarity.  (For instance, the 2023 Singing-Voice Conversion Challenge reported that top systems achieved near-human *naturalness* but still lagged far behind in *speaker similarity* ([arxiv.org](https://arxiv.org/html/2306.14422#:~:text=and%20in%20total%20we%20received,could%20reach%20a%20significant%20correlation)).)  In addition, **Papers With Code** tracks VC “leaderboards” on public datasets – for example, a LibriSpeech “test-clean” VC benchmark (where the top rated method is *kNN-VC (prematched HiFiGAN)*) ([paperswithcode.com](https://paperswithcode.com/task/voice-conversion/codeless?page=5#:~:text=Trend%20%20,See%C2%A0all)), and a similar VCTK benchmark.  Hugging Face itself hosts some VC-related datasets (e.g. DynamicSuperb’s **VoiceConversion_VCTK** test set, containing 2001 paired source/target utterances with transcripts and instructions ([www.selectdataset.com](https://www.selectdataset.com/dataset/730dfa10f18fd245a4d95d29509fd8ed#:~:text=,%E5%90%8D%E7%A7%B0%3A%20source_transcription))), but these are data repositories, not interactive leaderboards.  In summary, there is **no official HF leaderboard** for voice conversion yet; evaluations are done through research challenges and sites like PapersWithCode.  

Other benchmarks exist outside HF: the VCC datasets (from Interspeech 2016/18/20) are published (e.g. via Datashare or GitHub ([paperswithcode.com](https://paperswithcode.com/dataset/voice-conversion-challenge-2018#:~:text=2016%20was%20launched%20in%202016,The%20challenge))) and have defined evaluation (listening tests ranking MOS for naturalness/similarity).  PapersWithCode compiles VC datasets (VCC2018, etc.) and tracks best models on them.  For example, the VCC2018 challenge description describes how systems were evaluated on a limited (parallel/non-parallel) database and assessed by crowdsourced listening tests ([paperswithcode.com](https://paperswithcode.com/dataset/voice-conversion-challenge-2018#:~:text=2016%20was%20launched%20in%202016,The%20challenge)).  In practice, VC evaluation often combines subjective MOS scores and objective measures (mel-cepstral distortion, ASR word-error or speaker verification scores), but *subjective listening* remains the gold standard ([arxiv.org](https://arxiv.org/html/2306.14422#:~:text=and%20in%20total%20we%20received,could%20reach%20a%20significant%20correlation)) ([paperswithcode.com](https://paperswithcode.com/dataset/voice-conversion-challenge-2018#:~:text=2016%20was%20launched%20in%202016,The%20challenge)).  

# Creating a Hugging Face Voice Conversion Benchmark

To set up a VC benchmark *on* Hugging Face, one would use the Hub’s **leaderboard framework**.  Hugging Face provides a demo template (frontend & backend Spaces plus “requests” and “results” Datasets) that can be copied and customized ([github.com](https://github.com/huggingface/leaderboards/blob/main/docs/source/en/leaderboards/building_page.md#:~:text=,by%20the%20frontend%20for%20display)).  In this framework: 

- A **frontend Space** acts as the user interface (explaining the task, accepting submissions) and displays the leaderboard.  
- A **“requests” dataset** on the Hub stores submitted models’ outputs (e.g. converted audio samples) and status.  
- A **“results” dataset** stores evaluation scores for each submission. The backend updates this once it finishes computing the metrics.  
- An optional **backend Space** (often a Python service) polls the requests dataset, runs the evaluation code on new submissions, and writes results back ([huggingface.co](https://huggingface.co/demo-leaderboard-backend#:~:text=The%20space%20does%203%20things%3A)) ([github.com](https://github.com/huggingface/leaderboards/blob/main/docs/source/en/leaderboards/building_page.md#:~:text=,by%20the%20frontend%20for%20display)).  

To implement this for voice conversion, one would proceed as follows:

- **Prepare the data.**  Publish a Hugging Face Dataset of test utterances, with source audio (and possibly transcripts) and target speaker identities.  For example, one could use VCTK or LibriSpeech samples, pairing each source sentence with a target speaker.  (DynamicSuperb’s “VoiceConversion_VCTK” dataset is an example of such a test set on HF ([www.selectdataset.com](https://www.selectdataset.com/dataset/730dfa10f18fd245a4d95d29509fd8ed#:~:text=,%E5%90%8D%E7%A7%B0%3A%20source_transcription)).)  

- **Define the task and metrics.**  In the leaderboard’s `src/about.py`, you list one or more tasks (e.g. “any-to-one conversion”) and compute metrics. For VC, typical metrics include speaker similarity (e.g. cosine distance of speaker embeddings or a speaker ID accuracy) and content preservation (e.g. ASR word error rate on the converted audio).  One might also compute Mel-Cepstral Distortion or F0 correlation.  (These metrics can be implemented using Hugging Face’s [evaluate](https://huggingface.co/docs/evaluate/) library or custom code in the backend.)  

- **Configure the leaderboard.**  Fork Hugging Face’s leaderboard template and edit the required files.  Specifically, edit `src/envs.py` to set your organization, and `src/about.py` to define the task name, description, and metrics ([github.com](https://github.com/huggingface/leaderboards/blob/main/docs/source/en/leaderboards/building_page.md#:~:text=To%20get%20started%20on%20your,need%20to%20edit%202%20files)).  Then initialize the “results” dataset with a dummy entry so that the leaderboard shows up.  

- **Implement evaluation logic.**  In the backend Space (or in `main_backend.py` of the template), write code that loads each submission (converted audio), computes the defined metrics against the reference target, and writes scores to the results dataset.  As the HF demo notes, the Space will automatically “store submissions in requests, run evaluations (via `main_backend.py`), and display the results” ([huggingface.co](https://huggingface.co/demo-leaderboard-backend#:~:text=The%20space%20does%203%20things%3A)).  

- **Run and publish.**  Once configured, anyone can submit their converted audio (as audio files or serialized data) to the requests dataset (e.g. via Python or the UI), and the backend will evaluate and rank it.  The Cloud UI will then show the leaderboard table of models and scores.  

In short, creating an HF benchmark involves setting up two Datasets (requests/results) and two Spaces (frontend UI and optional backend runner) as per Hugging Face’s leaderboard guide ([github.com](https://github.com/huggingface/leaderboards/blob/main/docs/source/en/leaderboards/building_page.md#:~:text=,by%20the%20frontend%20for%20display)).  The key steps are defining the VC dataset and targets, writing or choosing appropriate audio-evaluation metrics, and integrating them into the Space’s code.  Once established, this benchmark will appear on the Hugging Face Model Hub under “Leaderboards” for voice conversion, much like existing others. 

**Sources:** Voice conversion challenges and datasets ([arxiv.org](https://arxiv.org/html/2306.14422#:~:text=In%20light%20of%20this%2C%20the,and%20to%20share%20views%20about)) ([paperswithcode.com](https://paperswithcode.com/dataset/voice-conversion-challenge-2018#:~:text=2016%20was%20launched%20in%202016,The%20challenge)) ([arxiv.org](https://arxiv.org/html/2306.14422#:~:text=and%20in%20total%20we%20received,could%20reach%20a%20significant%20correlation)) ([www.selectdataset.com](https://www.selectdataset.com/dataset/730dfa10f18fd245a4d95d29509fd8ed#:~:text=,%E5%90%8D%E7%A7%B0%3A%20source_transcription)); Hugging Face leaderboard documentation ([huggingface.co](https://huggingface.co/demo-leaderboard-backend#:~:text=The%20space%20does%203%20things%3A)) ([github.com](https://github.com/huggingface/leaderboards/blob/main/docs/source/en/leaderboards/building_page.md#:~:text=,by%20the%20frontend%20for%20display)); PapersWithCode benchmarks ([paperswithcode.com](https://paperswithcode.com/task/voice-conversion/codeless?page=5#:~:text=Trend%20%20,See%C2%A0all)) (as cited above).