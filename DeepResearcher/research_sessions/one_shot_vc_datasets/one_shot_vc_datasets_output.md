# Dataset Comparison (VCTK vs LibriTTS vs AISHELL-3)

- **VCTK:** Contains about 109–110 native English speakers ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/3443#:~:text=This%20CSTR%20VCTK%20Corpus%20includes,https%3A%2F%2Fdoi.org%2F10.1109%2FICSDA.2013.6709856%20The%20rainbow%20passage)). It was recorded in a controlled studio (originally at 96 kHz, 16-bit) ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/2651?show=full#:~:text=transcripts%20of%20the%20speech%20are,en_UK)). Speakers read scripted sentences (newspaper text, the “Rainbow” passage, etc.) ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/3443#:~:text=This%20CSTR%20VCTK%20Corpus%20includes,https%3A%2F%2Fdoi.org%2F10.1109%2FICSDA.2013.6709856%20The%20rainbow%20passage)), so the style is out-loud reading rather than conversational. VCTK includes various British English accents and both genders (roughly balanced), though exact male/female split isn’t documented here. Speakers are mostly adults (ages roughly 18–60) with some accent variations. Because it’s read speech and all monolingual English, language style is consistent but narrow (no spontaneous or conversational speech).  

- **LibriTTS:** Contains ~585 hours of read English speech at 24 kHz ([us.openslr.org](https://us.openslr.org/resources/60/about.html#:~:text=LibriTTS%20is%20a%20multi,of%20the%20LibriSpeech%20corpus)). It is derived from LibriVox audiobooks (the LibriSpeech corpus); hence it implicitly covers all ~2,456 LibriSpeech speakers (mostly adult audiobook readers) ([us.openslr.org](https://us.openslr.org/resources/60/about.html#:~:text=LibriTTS%20is%20a%20multi,of%20the%20LibriSpeech%20corpus)) ([www.researchgate.net](https://www.researchgate.net/publication/371163309_LibriTTS-R_A_Restored_Multi-Speaker_Text-to-Speech_Corpus#:~:text=This%20paper%20introduces%20a%20new,data%20at%2024%20kHz%20sampling)). Recording quality is good but variable: LibriVox recordings were cleaned and standardized, but each speaker recorded in different conditions. LibriTTS excludes very short or noisy clips, but moderate noise and channel differences remain. All speech is scripted (book chapters), so again “read” style, with the natural prosody of narrated audiobooks. American/British English accents dominate (LibriVox is international but skewed to certain novel readers), and most speakers are adults. Gender balance tends to be male-heavy (many classic novels were read by men), but detailed breakdown isn’t given here. Because of its large number of speakers (thousands) and wide coverage of English dialects, LibriTTS offers great speaker diversity at the cost of style (no casual dialogue, all formal reading) ([us.openslr.org](https://us.openslr.org/resources/60/about.html#:~:text=LibriTTS%20is%20a%20multi,of%20the%20LibriSpeech%20corpus)) ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/3443#:~:text=This%20CSTR%20VCTK%20Corpus%20includes,https%3A%2F%2Fdoi.org%2F10.1109%2FICSDA.2013.6709856%20The%20rainbow%20passage)).  

- **AISHELL-3:** Contains about 85 hours from 218 native Mandarin Chinese speakers ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=tains%20roughly%2085%20hours%20of,neutral%20recordings%20spanning)). Per the corpus paper, 175 are female and 43 male; 165 are from northern China and 51 from southern regions ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=gender%20175%20female%20%2F%2043,male)). Additionally, 175 speakers are under 25 years old and only 43 over 25 ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=gender%20175%20female%20%2F%2043,male)). Recording quality is very high: all sessions used the same professional indoor setup (22 kHz or 44.1 kHz, 16-bit) with low noise and no echoes ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=recording%20is%20set%20up%20in,indoor%20environments%20with%20no%20sig)). Speakers read neutral scripted prompts (no strong emotion) ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=recording%20is%20set%20up%20in,indoor%20environments%20with%20no%20sig)). Thus, AISHELL-3 has very consistent audio but is heavily skewed toward young, female, northern Mandarin speakers. There is no conversational style or other languages – only planned reading in Mandarin with a relatively restricted demographic profile.

**Summary of (1) Speaker Diversity:** VCTK has ~110 English speakers (gender roughly balanced, various UK accents, broad adult ages) ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/3443#:~:text=This%20CSTR%20VCTK%20Corpus%20includes,https%3A%2F%2Fdoi.org%2F10.1109%2FICSDA.2013.6709856%20The%20rainbow%20passage)). LibriTTS has **several thousand** English speakers (from diverse backgrounds via LibriVox) ([us.openslr.org](https://us.openslr.org/resources/60/about.html#:~:text=LibriTTS%20is%20a%20multi,of%20the%20LibriSpeech%20corpus)), giving it the largest variety of voices. AISHELL-3 has 218 Chinese speakers but is heavily female (≈80% female) and young ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=gender%20175%20female%20%2F%2043,male)) ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=gender%20175%20female%20%2F%2043,male)). VCTK and LibriTTS cover English; AISHELL-3 covers Mandarin only.

**(2) Recording Quality & Consistency:** VCTK was recorded in a lab with high-fidelity microphones (96 kHz released versions available) ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/2651?show=full#:~:text=transcripts%20of%20the%20speech%20are,en_UK)). It’s generally clean and uniform. LibriTTS was compiled from many audiobook recordings, all resampled to 24 kHz ([us.openslr.org](https://us.openslr.org/resources/60/about.html#:~:text=LibriTTS%20is%20a%20multi,of%20the%20LibriSpeech%20corpus)); quality is high on average but varies by speaker/equipment (background hiss or microphone differences can occur). AISHELL-3 was recorded in a single controlled environment (48 kHz originally, downsampled to 22 kHz) ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=recording%20is%20set%20up%20in,indoor%20environments%20with%20no%20sig)); all files are clean with minimal noise or reverb. In summary, VCTK and AISHELL-3 have very consistent studio conditions, while LibriTTS has more recording variability (but also is larger-scaled). 

**(3) Speaking Style Variety:** All three are *read speech* corpora – none contain natural conversational dialogue. VCTK and AISHELL-3 speakers read prepared scripts in a neutral tone ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/3443#:~:text=This%20CSTR%20VCTK%20Corpus%20includes,https%3A%2F%2Fdoi.org%2F10.1109%2FICSDA.2013.6709856%20The%20rainbow%20passage)) ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=recording%20is%20set%20up%20in,indoor%20environments%20with%20no%20sig)). LibriTTS speakers read book/text passages (audiobooks) ([us.openslr.org](https://us.openslr.org/resources/60/about.html#:~:text=LibriTTS%20is%20a%20multi,of%20the%20LibriSpeech%20corpus)). Thus, style variation is limited to reading prosody only. We should note that VCTK includes an “elicitation paragraph” to capture accents ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/3443#:~:text=This%20CSTR%20VCTK%20Corpus%20includes,https%3A%2F%2Fdoi.org%2F10.1109%2FICSDA.2013.6709856%20The%20rainbow%20passage)), but it’s still reading. In practice, all three lack spontaneous or emotive speech styles.

**(4) Generalization to Unseen Speakers:** LibriTTS’s huge speaker count (thousands) and variety of voices make it best suited to generalize to new, unseen English speakers. Models trained on LibriTTS will see a broad range of vocal timbres, accents, and pronunciations. VCTK (≈110 speakers ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/3443#:~:text=This%20CSTR%20VCTK%20Corpus%20includes,https%3A%2F%2Fdoi.org%2F10.1109%2FICSDA.2013.6709856%20The%20rainbow%20passage))) is much smaller; a VC model trained only on VCTK may overfit to that limited set of British-style voices. AISHELL-3 (218 speakers) offers moderate diversity, but its language/cultural focus (Mandarin) limits generalization to other languages. In practice, for an English one-shot VC model, LibriTTS would generalize best to new English voices, VCTK second (but might bias toward UK accents), and AISHELL-3 is mainly useful if converting Mandarin voices.  

**(5) Known Issues or Biases:**  
- *VCTK:* While accents are “various” ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/3443#:~:text=This%20CSTR%20VCTK%20Corpus%20includes,https%3A%2F%2Fdoi.org%2F10.1109%2FICSDA.2013.6709856%20The%20rainbow%20passage)), they are largely English (UK) speaking; accents like American English, Asian English, etc., are underrepresented. The dataset uses formal text (newspaper, scripted paragraphs), so models may not handle informal speech well. Gender balance is unclear but generally moderate. No major label biases are documented, but one speaker’s transcript is missing (p315) ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/2651?show=full#:~:text=transcripts%20of%20the%20speech%20are,en_UK)).  
- *LibriTTS:* Drawn from LibriVox, it has a bias toward audiobook readers. This often means older, classic literature styles and possibly more male voices (many older male narrators) and an American/British accent bias. LibriTTS filters out noisy samples ([us.openslr.org](https://us.openslr.org/resources/60/about.html#:~:text=LibriTTS%20is%20a%20multi,of%20the%20LibriSpeech%20corpus)), so very noisy environments aren’t represented. Content bias: formal written text (classic novels), no slang by design. As an effect, a model may struggle with slang, dialects outside the corpus, or casual speech.  
- *AISHELL-3:* Skewed demographically: mostly young (<25) female Mandarin speakers ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=gender%20175%20female%20%2F%2043,male)) ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=gender%20175%20female%20%2F%2043,male)). Few male and few older voices mean models may underperform on these groups. It covers Northern vs Southern Mandarin accents (165 vs 51) ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=gender%20175%20female%20%2F%2043,male)), but no non-Mandarin or non-native voices. Being scripted and read, it also lacks conversational style. The corpus notes some speakers were “hesitant or mechanical” when unsure of scripts (amid amateur recording conditions), so not all recordings are equally fluent; this can introduce style artifacts. 

## Data Augmentation Strategies

To improve one-shot VC robustness/generalization, it’s common to apply audio augmentation during training. Effective techniques include:

- **Pitch Shifting:** Randomly raise or lower the fundamental frequency of training utterances (e.g. ± a few semitones). This simulates speakers with higher or lower voice pitch (for example, male vs. child voices). Time-invariant pitch shifts help the model learn to disentangle pitch from timbre. Prior work notes that **pitch shifting** (and **time-stretching**) are “invaluable tools” for audio augmentation ([d2wozrt205r2fu.cloudfront.net](https://d2wozrt205r2fu.cloudfront.net/p/data-augmentation-knowledge-speech-recognition-cat-ai#:~:text=Both%20time%20stretching%20and%20pitch,applications%2C%20particularly%20in%20data%20augmentation)).  

- **Time Stretching (Speed Perturbation):** Slightly speed up or slow down utterances (e.g. ±10–20% duration change) without altering pitch. This creates variation in speaking rate. Combined with pitch shift, it exposes the model to tempo and prosody differences. The cited survey emphasizes that both time stretching and pitch shifting are widely used in augmentation to cover variations that machines must tolerate ([d2wozrt205r2fu.cloudfront.net](https://d2wozrt205r2fu.cloudfront.net/p/data-augmentation-knowledge-speech-recognition-cat-ai#:~:text=Both%20time%20stretching%20and%20pitch,applications%2C%20particularly%20in%20data%20augmentation)).

- **Noise Injection:** Add background noise or competing sounds to the speech (e.g. urban noise, music, crowd noise at various SNRs). Injecting diverse noise types makes the model robust to real-world conditions. Studies on speech augmentation confirm that adding varied noise significantly improves robustness ([d2wozrt205r2fu.cloudfront.net](https://d2wozrt205r2fu.cloudfront.net/p/data-augmentation-knowledge-speech-recognition-cat-ai#:~:text=By%20employing%20these%20noise%20addition,For%20further%20details%20on)). For VC, adding noise helps the model learn to ignore irrelevant acoustic conditions. Common practice is to use noise corpora (e.g. MUSAN) or real-world recordings to perturb the clean speech. 

- **Room Reverberation:** Convolve signals with simulated room impulse responses (RIRs) to introduce reverberation and simulate different acoustic spaces. This augmentation forces the model to handle echo and distance variations. While less discussed in VC literature specifically, reverberation augmentation is standard in robust speech systems; it broadens the acoustic distortions encountered during training. (The general augmentation research highlights “preparing models for varied acoustic environments” ([d2wozrt205r2fu.cloudfront.net](https://d2wozrt205r2fu.cloudfront.net/p/data-augmentation-knowledge-speech-recognition-cat-ai#:~:text=By%20employing%20these%20noise%20addition,For%20further%20details%20on)).)

- **Other Augmentations:** Volume perturbation (random gain changes), spectral shaping (equalization), and SpecAugment-style masks (time/frequency masking) can also help. In practice, simple augmentations suffice. As one recent study on VC notes, even **“simple data augmentation techniques”** combined with diverse training data vastly improved VC robustness to new accents, noise, and recording conditions ([www.isca-archive.org](https://www.isca-archive.org/interspeech_2023/tanna23_interspeech.html#:~:text=frustrated%20users,data%20augmentation%20techniques%20in%20pretraining)). In short, applying these augmentations (pitch/time warping, noise, reverb, etc.) during VC model training has been shown to yield more stable, generalized conversion on unseen voices and conditions ([www.isca-archive.org](https://www.isca-archive.org/interspeech_2023/tanna23_interspeech.html#:~:text=frustrated%20users,data%20augmentation%20techniques%20in%20pretraining)) ([d2wozrt205r2fu.cloudfront.net](https://d2wozrt205r2fu.cloudfront.net/p/data-augmentation-knowledge-speech-recognition-cat-ai#:~:text=By%20employing%20these%20noise%20addition,For%20further%20details%20on)) ([d2wozrt205r2fu.cloudfront.net](https://d2wozrt205r2fu.cloudfront.net/p/data-augmentation-knowledge-speech-recognition-cat-ai#:~:text=Both%20time%20stretching%20and%20pitch,applications%2C%20particularly%20in%20data%20augmentation)). 

**Sources:** The above comparisons and guidelines draw on dataset documentation and VC literature (e.g., VCTK and AISHELL-3 corpus papers ([datashare.ed.ac.uk](https://datashare.ed.ac.uk/handle/10283/3443#:~:text=This%20CSTR%20VCTK%20Corpus%20includes,https%3A%2F%2Fdoi.org%2F10.1109%2FICSDA.2013.6709856%20The%20rainbow%20passage)) ([www.researchgate.net](https://www.researchgate.net/publication/354221815_AISHELL-3_A_Multi-Speaker_Mandarin_TTS_Corpus#:~:text=gender%20175%20female%20%2F%2043,male)), the LibriTTS description ([us.openslr.org](https://us.openslr.org/resources/60/about.html#:~:text=LibriTTS%20is%20a%20multi,of%20the%20LibriSpeech%20corpus)), and recent studies on VC data augmentation ([www.isca-archive.org](https://www.isca-archive.org/interspeech_2023/tanna23_interspeech.html#:~:text=frustrated%20users,data%20augmentation%20techniques%20in%20pretraining)) ([d2wozrt205r2fu.cloudfront.net](https://d2wozrt205r2fu.cloudfront.net/p/data-augmentation-knowledge-speech-recognition-cat-ai#:~:text=By%20employing%20these%20noise%20addition,For%20further%20details%20on)) ([d2wozrt205r2fu.cloudfront.net](https://d2wozrt205r2fu.cloudfront.net/p/data-augmentation-knowledge-speech-recognition-cat-ai#:~:text=Both%20time%20stretching%20and%20pitch,applications%2C%20particularly%20in%20data%20augmentation))).